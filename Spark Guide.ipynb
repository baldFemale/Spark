{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName(\"Guide\")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BASIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"in.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "# lineLengths.persist() to keep lineLengths in cache -> the difference between RDD and Map&Reduce\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Passing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        # copy field into local variable in order to avoid send the whole class object but just field\n",
    "        field = self.field\n",
    "        return rdd.map(lambda s: field + s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Understanding Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    # still local counter on each executor due to closure\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Transformation(params) | Meaning |\n",
    "|-----|-----|\n",
    "| map(func) | literal meaning |\n",
    "| filter(func) | literal meaning |\n",
    "| flatMap(func) | map + flatten |\n",
    "| mapPartitions(func of generator) | map in partitions |\n",
    "| mapPartitionsWithIndex(func of generator) | map in partitions with corresponding index | \n",
    "| sample(withReplacement, fration, seed) | literal meaning |\n",
    "| union(dataset) | literal meaning |\n",
    "| intersection(dataset) | literal meaning |\n",
    "| distinct | return a new dataset with unique element |\n",
    "| groupByKey | literal meaning |\n",
    "| reduceByKey(func) | literal meaning |\n",
    "| aggregateByKey(seqOP, combOP) | aggregate based on self-defined function |\n",
    "| sortByKey | literal meaning |\n",
    "| join(dataset) | literal meaning |\n",
    "| cogroup(dataset) | group within two dataset |\n",
    "| cartesian(dataset | cartesian product |\n",
    "| pipe(command) | execute bash script |\n",
    "| coalece(number) | reduce number |\n",
    "| repartition(number) | reshuffle the data |\n",
    "| repartitionAndSortWithPartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Action | Meaning |\n",
    "| ----- | ----- |\n",
    "| reduce(func) | literal meaning | \n",
    "| collect | return all element |\n",
    "| count | literal meaning |\n",
    "| first | literal meaning |\n",
    "| take(n) | first()==take(1) |\n",
    "| takeSample(withReplacement, number, seed) | literal meaning |\n",
    "| takeOrdered | take with a custom comparator |\n",
    "| saveAsTextFile(path) | literal meaning |\n",
    "| saveAsSequenceFile(path) | literal meaning |\n",
    "| saveAsObjectFile(path) | literal meaning |\n",
    "| countByKey() | literal meaning |\n",
    "| foreach() | literal meaning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RDD Persisence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Shared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* boardcast -> read only variable\n",
    "* accumulators -> added only variable -> activated by action thus each part only calculated once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo1->wordcount\n",
    "text_file = sc.textFile(\"hdfs://...\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo2->estimate Pi\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(xrange(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get hands dirty\n",
    "* Task: Write a Spark program to find the top 10 products based on the number of user reviews.\n",
    "* Dataset: Use the Musical Instruments review file (5-core) and metadata from the Amazon product dataset (http://jmcauley.ucsd.edu/data/amazon/links.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlcontext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique reviewerID for each product\n",
    "review = sqlcontext.read.schema(\"asin String, reviewerID String\").json (\"./Musical_Instruments_5.json\").select([\"asin\",\"reviewerID\"])\n",
    "asin_ids = review.rdd.map(lambda x: (x.asin,x.reviewerID)).distinct()\n",
    "asin_ids = asin_ids.map(lambda x: (x[0],1)).reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B003VWJ2K8', 163),\n",
       " ('B0002E1G5C', 143),\n",
       " ('B0002F7K7Y', 116),\n",
       " ('B003VWKPHC', 114),\n",
       " ('B0002H0A3S', 93),\n",
       " ('B0002CZVXM', 74),\n",
       " ('B0006NDF8A', 71),\n",
       " ('B0009G1E0K', 69),\n",
       " ('B0002E2KPC', 68),\n",
       " ('B0002GLDQM', 67)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asin_ids.sortBy(lambda x:-x[1]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product-price pair\n",
    "product = sqlcontext.read.json(\"./meta_Musical_Instruments.json\")\n",
    "product_price = product.rdd.map(lambda x:(x.asin,x.price))\n",
    "product_price = product_price.groupByKey().map(lambda x: (x[0],list(set(x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B0002E1G5C', ([6.92], 143)),\n",
       " ('B0002F7K7Y', ([1.96], 116)),\n",
       " ('B0002H0A3S', ([0.2], 93)),\n",
       " ('B0002CZVXM', ([10.22], 74)),\n",
       " ('B0006NDF8A', ([10.99], 71)),\n",
       " ('B0009G1E0K', ([6.46], 69)),\n",
       " ('B0002E2KPC', ([9.92], 68)),\n",
       " ('B0002GLDQM', ([1.54], 67)),\n",
       " ('B004XNK7AI', ([9.95], 65)),\n",
       " ('B005FKF1PY', ([12.56], 63))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_price.join(asin_ids).sortBy(lambda x: -x[1][1]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top10 products based on the number of unique reviewers. \n",
    "However, bug remains in reading json which ignore several records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
